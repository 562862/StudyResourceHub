# 自动化人工智能期末复习笔记

## 目录
1. [人工智能基础概念](#1-人工智能基础概念)
2. [机器学习基础](#2-机器学习基础)
3. [监督学习](#3-监督学习)
4. [神经网络与深度学习](#4-神经网络与深度学习)
5. [无监督学习](#5-无监督学习)
6. [强化学习](#6-强化学习)
7. [自动化应用场景](#7-自动化应用场景)

---

## 1. 人工智能基础概念

### 1.1 什么是人工智能？
**定义**：人工智能(Artificial Intelligence, AI)是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。

**核心目标**：让机器像人一样思考、学习、决策和解决问题。

**AI的三个层次**：
- **弱人工智能(ANI)**：专注于特定任务（如语音识别、图像分类）
- **强人工智能(AGI)**：具有人类级别的通用智能（目前尚未实现）
- **超人工智能(ASI)**：超越人类智能的AI（理论阶段）

### 1.2 AI的发展历程
- **1956年**：达特茅斯会议，AI诞生
- **1980-1990年代**：专家系统兴起
- **2010年代至今**：深度学习革命，大数据驱动

---

## 2. 机器学习基础

### 2.1 什么是机器学习？
**定义**：机器学习是一种让计算机从数据中自动学习规律，而不需要明确编程的方法。

**核心思想**：通过大量数据训练模型，使其能够对新数据做出预测或决策。

### 2.2 机器学习的分类

#### (1) 监督学习 (Supervised Learning)
- **特点**：训练数据有标签（正确答案）
- **目标**：学习输入到输出的映射关系
- **应用**：分类、回归

#### (2) 无监督学习 (Unsupervised Learning)
- **特点**：训练数据没有标签
- **目标**：发现数据中的隐藏结构
- **应用**：聚类、降维

#### (3) 强化学习 (Reinforcement Learning)
- **特点**：通过与环境交互获得奖励或惩罚
- **目标**：学习最优策略
- **应用**：游戏AI、机器人控制

### 2.3 核心概念

**特征 (Feature)**：描述数据的属性或变量
- 例：预测房价时，特征包括面积、地段、房龄等

**标签 (Label)**：监督学习中的"正确答案"
- 例：房价预测中的实际价格

**训练集 (Training Set)**：用于训练模型的数据

**测试集 (Test Set)**：用于评估模型性能的数据

**过拟合 (Overfitting)**：模型在训练集上表现很好，但在测试集上表现差
- 原因：模型过于复杂，记住了训练数据的噪声

**欠拟合 (Underfitting)**：模型在训练集和测试集上都表现差
- 原因：模型过于简单，无法捕捉数据规律

---

## 3. 监督学习

### 3.1 线性回归 (Linear Regression)

#### 基本原理
预测连续值输出，假设输入和输出之间存在线性关系。

**数学模型**：
```
y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
```
- y：预测值
- x₁, x₂, ..., xₙ：特征
- w₁, w₂, ..., wₙ：权重（参数）
- b：偏置（截距）

**损失函数**（均方误差 MSE）：
```
MSE = (1/m) Σ(yᵢ - ŷᵢ)²
```
- m：样本数量
- yᵢ：真实值
- ŷᵢ：预测值

#### 例题1：简单线性回归

**问题**：根据学习时间预测考试成绩

| 学习时间(小时) | 考试成绩(分) |
|---------------|-------------|
| 1             | 50          |
| 2             | 55          |
| 3             | 65          |
| 4             | 70          |
| 5             | 85          |

假设学习到的模型为：成绩 = 8.5 × 学习时间 + 42

**问：如果学习6小时，预测成绩是多少？**

**解答**：
```
成绩 = 8.5 × 6 + 42
     = 51 + 42
     = 93分
```

**问：计算前3个样本的MSE**

**解答**：
```
样本1预测值：8.5×1 + 42 = 50.5，误差²=(50-50.5)² = 0.25
样本2预测值：8.5×2 + 42 = 59，误差²=(55-59)² = 16
样本3预测值：8.5×3 + 42 = 67.5，误差²=(65-67.5)² = 6.25

MSE = (0.25 + 16 + 6.25) / 3 = 7.5
```

### 3.2 逻辑回归 (Logistic Regression)

#### 基本原理
用于二分类问题，输出是概率值（0到1之间）。

**数学模型**：
```
P(y=1|x) = 1 / (1 + e^(-z))
其中 z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
```

这个函数叫做**Sigmoid函数**，将线性输出转换为概率。

#### 例题2：垃圾邮件分类

**问题**：根据邮件特征判断是否为垃圾邮件

特征：
- x₁：包含"中奖"词汇的次数
- x₂：包含链接的数量

假设学习到的模型为：z = 0.8x₁ + 0.5x₂ - 2

**问：一封邮件包含3次"中奖"，2个链接，是垃圾邮件的概率是多少？**

**解答**：
```
步骤1：计算z
z = 0.8×3 + 0.5×2 - 2
  = 2.4 + 1 - 2
  = 1.4

步骤2：计算概率
P(垃圾邮件) = 1 / (1 + e^(-1.4))
            = 1 / (1 + 0.2466)
            = 1 / 1.2466
            ≈ 0.802

答：该邮件是垃圾邮件的概率约为80.2%
```

**决策规则**：通常P≥0.5判定为垃圾邮件，P<0.5判定为正常邮件。

### 3.3 决策树 (Decision Tree)

#### 基本原理
通过一系列if-else规则进行分类或回归，像一个倒置的树结构。

**关键概念**：
- **根节点**：最顶层的节点
- **内部节点**：决策节点（问问题）
- **叶节点**：最终决策结果
- **信息增益**：选择最佳分割特征的标准

#### 例题3：是否打网球决策

**数据**：
| 天气  | 温度 | 湿度 | 风速 | 打网球? |
|-------|------|------|------|---------|
| 晴天  | 高   | 高   | 弱   | 否      |
| 晴天  | 高   | 高   | 强   | 否      |
| 阴天  | 高   | 高   | 弱   | 是      |
| 雨天  | 中   | 高   | 弱   | 是      |
| 雨天  | 低   | 正常 | 弱   | 是      |
| 雨天  | 低   | 正常 | 强   | 否      |
| 阴天  | 低   | 正常 | 强   | 是      |

**构建的决策树**（简化版）：
```
                天气?
               /  |  \
            晴天 阴天 雨天
            /    |    \
          湿度?  是   风速?
         /  \        /  \
       高   正常    弱   强
       /     \      /    \
      否      是    是    否
```

**问：如果天气=晴天，湿度=正常，预测是否打网球？**

**解答**：
```
步骤1：从根节点开始，天气=晴天，走左分支
步骤2：到达"湿度?"节点，湿度=正常，走右分支
步骤3：到达叶节点"是"

答：预测会打网球
```

### 3.4 支持向量机 (SVM)

#### 基本原理
找到一个最优的超平面（决策边界），使得两类数据之间的间隔最大。

**核心概念**：
- **支持向量**：离决策边界最近的数据点
- **间隔(Margin)**：决策边界到支持向量的距离
- **核函数**：处理非线性可分问题

#### 例题4：二维平面分类

**问题**：两类数据点如下
- 类别A：(1,1), (2,2), (2,0)
- 类别B：(0,0), (1,0), (0,1)

假设找到的决策边界为：x₁ + x₂ = 2.5

**问：点(3,1)属于哪一类？**

**解答**：
```
步骤1：将点代入决策函数
f(3,1) = 3 + 1 = 4

步骤2：判断
因为 4 > 2.5，点在决策边界上方

规则：f(x) > 2.5 判定为类别A
     f(x) < 2.5 判定为类别B

答：点(3,1)属于类别A
```

### 3.5 模型评估指标

#### 分类问题评估

**混淆矩阵**：
```
                预测为正  预测为负
实际为正         TP       FN
实际为负         FP       TN
```
- TP (True Positive)：真正例
- FN (False Negative)：假负例
- FP (False Positive)：假正例
- TN (True Negative)：真负例

**准确率 (Accuracy)**：
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**精确率 (Precision)**：
```
Precision = TP / (TP + FP)
```
含义：预测为正的样本中，真正为正的比例

**召回率 (Recall)**：
```
Recall = TP / (TP + FN)
```
含义：实际为正的样本中，被正确预测的比例

**F1分数**：
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

#### 例题5：医疗诊断评估

**问题**：一个疾病检测模型对100个病人进行诊断：
- 实际患病：30人
- 实际健康：70人
- 预测患病且实际患病：25人
- 预测患病但实际健康：10人
- 预测健康但实际患病：5人
- 预测健康且实际健康：60人

**问：计算准确率、精确率、召回率和F1分数**

**解答**：
```
首先构建混淆矩阵：
                预测患病  预测健康
实际患病         25 (TP)  5 (FN)
实际健康         10 (FP)  60 (TN)

1. 准确率
Accuracy = (25 + 60) / 100 = 0.85 = 85%

2. 精确率
Precision = 25 / (25 + 10) = 25/35 ≈ 0.714 = 71.4%
含义：在预测患病的35人中，25人真的患病

3. 召回率
Recall = 25 / (25 + 5) = 25/30 ≈ 0.833 = 83.3%
含义：在实际患病的30人中，25人被检测出来

4. F1分数
F1 = 2 × (0.714 × 0.833) / (0.714 + 0.833)
   = 2 × 0.595 / 1.547
   ≈ 0.769 = 76.9%
```

**解读**：
- 准确率85%看起来不错
- 但召回率83.3%意味着有5个患病者被漏诊（这在医疗中很严重！）
- 在医疗场景中，召回率比精确率更重要

---

## 4. 神经网络与深度学习

### 4.1 神经网络基础

#### 什么是神经网络？
模拟人脑神经元结构的计算模型，由大量简单的神经元相互连接组成。

**基本组成**：
1. **输入层**：接收原始数据
2. **隐藏层**：进行特征提取和变换
3. **输出层**：产生最终结果

**单个神经元的工作原理**：
```
输入: x₁, x₂, ..., xₙ
权重: w₁, w₂, ..., wₙ
偏置: b

步骤1：加权求和
z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b

步骤2：激活函数
a = f(z)

常用激活函数：
- ReLU: f(z) = max(0, z)
- Sigmoid: f(z) = 1/(1+e^(-z))
- Tanh: f(z) = (e^z - e^(-z))/(e^z + e^(-z))
```

#### 例题6：简单神经元计算

**问题**：一个神经元有3个输入，使用ReLU激活函数
- 输入：x₁=2, x₂=-1, x₃=3
- 权重：w₁=0.5, w₂=0.3, w₃=-0.2
- 偏置：b=1

**问：计算该神经元的输出**

**解答**：
```
步骤1：计算加权和
z = 0.5×2 + 0.3×(-1) + (-0.2)×3 + 1
  = 1 - 0.3 - 0.6 + 1
  = 1.1

步骤2：应用ReLU激活函数
ReLU(z) = max(0, 1.1) = 1.1

答：该神经元的输出为1.1
```

### 4.2 反向传播算法

#### 基本原理
通过计算损失函数对每个权重的梯度，逐层向后传播，更新网络参数。

**训练步骤**：
1. **前向传播**：输入数据，计算预测值
2. **计算损失**：比较预测值和真实值
3. **反向传播**：计算梯度
4. **参数更新**：根据梯度调整权重和偏置

**梯度下降公式**：
```
w = w - α × ∂Loss/∂w
```
- α：学习率（控制更新步长）
- ∂Loss/∂w：损失函数对权重的梯度

#### 例题7：梯度下降更新

**问题**：某权重w=2，损失函数对w的梯度为-0.5，学习率α=0.1

**问：更新后的权重是多少？连续更新3次后的权重？**

**解答**：
```
第1次更新：
w_new = 2 - 0.1 × (-0.5) = 2 + 0.05 = 2.05

第2次更新：（假设梯度仍为-0.5）
w_new = 2.05 - 0.1 × (-0.5) = 2.05 + 0.05 = 2.10

第3次更新：
w_new = 2.10 - 0.1 × (-0.5) = 2.10 + 0.05 = 2.15

答：
第1次更新后：w=2.05
第3次更新后：w=2.15

解释：梯度为负说明损失函数在该点递减，
所以权重应该增大（沿梯度相反方向）
```

### 4.3 卷积神经网络 (CNN)

#### 基本原理
专门用于处理图像数据的神经网络，通过卷积操作提取图像特征。

**核心组件**：
1. **卷积层 (Convolutional Layer)**：
   - 用卷积核（滤波器）扫描图像
   - 提取局部特征（边缘、纹理等）

2. **池化层 (Pooling Layer)**：
   - 降低特征图的维度
   - 常用最大池化(Max Pooling)

3. **全连接层 (Fully Connected Layer)**：
   - 将特征映射到最终输出

#### 例题8：卷积操作

**问题**：有一个3×3的输入图像和2×2的卷积核

输入图像：
```
1  2  3
4  5  6
7  8  9
```

卷积核：
```
1  0
-1 0
```

**问：使用步长(stride)=1，无填充(padding)，计算卷积结果**

**解答**：
```
卷积操作：将卷积核与图像局部区域逐元素相乘后求和

位置(0,0)：左上角2×2区域
1×1 + 2×0 + 4×(-1) + 5×0 = 1 - 4 = -3

位置(0,1)：往右移一格
2×1 + 3×0 + 5×(-1) + 6×0 = 2 - 5 = -3

位置(1,0)：往下移一格
4×1 + 5×0 + 7×(-1) + 8×0 = 4 - 7 = -3

位置(1,1)：右下角2×2区域
5×1 + 6×0 + 8×(-1) + 9×0 = 5 - 8 = -3

卷积结果（特征图）：
-3  -3
-3  -3

说明：这个卷积核检测垂直边缘
     （原图像是递增的，所以结果是负值）
```

#### 例题9：最大池化

**问题**：对以下4×4特征图进行2×2最大池化，步长=2

输入：
```
1  3  2  4
5  6  7  8
9  10 11 12
13 14 15 16
```

**问：池化后的结果是什么？**

**解答**：
```
最大池化：在每个池化窗口中取最大值

左上2×2区域：
1  3
5  6
最大值 = 6

右上2×2区域：
2  4
7  8
最大值 = 8

左下2×2区域：
9  10
13 14
最大值 = 14

右下2×2区域：
11 12
15 16
最大值 = 16

池化结果：
6   8
14  16

作用：
1. 降维（从4×4变为2×2）
2. 保留主要特征
3. 增强平移不变性
```

### 4.4 循环神经网络 (RNN)

#### 基本原理
专门处理序列数据（如文本、时间序列），具有记忆功能。

**核心特点**：
- 输出不仅依赖当前输入，还依赖之前的输入
- 有隐藏状态(hidden state)存储历史信息

**RNN计算公式**：
```
h_t = f(W_h × h_{t-1} + W_x × x_t + b)
y_t = W_y × h_t + b_y
```
- h_t：t时刻的隐藏状态
- x_t：t时刻的输入
- y_t：t时刻的输出

#### 例题10：情感分析

**问题**：使用RNN分析句子情感："这部电影很好看"

假设模型处理流程：
1. 输入：这 → 隐藏状态h₁=[0.2]
2. 输入：部 → 隐藏状态h₂=[0.3]
3. 输入：电影 → 隐藏状态h₃=[0.5]
4. 输入：很 → 隐藏状态h₄=[0.7]
5. 输入：好看 → 隐藏状态h₅=[0.9]

最终输出：正面情感概率=0.92，负面情感概率=0.08

**问：解释RNN如何理解这句话？**

**解答**：
```
时间步1："这"
- 刚开始处理，信息量少，h₁=0.2

时间步2："这部"
- 开始形成词组概念，h₂=0.3

时间步3："这部电影"
- 识别出主体是"电影"，h₃=0.5

时间步4："这部电影很"
- "很"是程度副词，提示有评价，h₄=0.7

时间步5："这部电影很好看"
- "好看"是正面词汇，确定正面情感，h₅=0.9

结论：正面情感（92%概率）

RNN的优势：
- 能理解词序（"不好看"和"好看"意思相反）
- 能捕捉上下文关系
- 隐藏状态不断更新，积累信息
```

---

## 5. 无监督学习

### 5.1 K-均值聚类 (K-Means)

#### 基本原理
将数据分成K个簇，使得同一簇内的数据点尽可能相似，不同簇的数据点尽可能不同。

**算法步骤**：
1. 随机初始化K个聚类中心
2. 将每个数据点分配到最近的聚类中心
3. 更新聚类中心（计算每个簇内所有点的平均值）
4. 重复步骤2-3，直到聚类中心不再变化

**距离计算**（欧氏距离）：
```
d = √[(x₁-y₁)² + (x₂-y₂)² + ... + (xₙ-yₙ)²]
```

#### 例题11：客户分群

**问题**：有5个客户的购买数据（消费金额，购买频率）

| 客户 | 消费金额(百元) | 购买频率(次/月) |
|------|---------------|----------------|
| A    | 10            | 2              |
| B    | 12            | 3              |
| C    | 50            | 10             |
| D    | 52            | 9              |
| E    | 11            | 2              |

使用K=2进行聚类，初始中心：C₁=(10,2), C₂=(50,10)

**问：第一轮迭代后的聚类结果和新的聚类中心**

**解答**：
```
第一轮：分配数据点到最近的中心

客户A(10,2)到C₁的距离：
d₁ = √[(10-10)² + (2-2)²] = 0

客户A(10,2)到C₂的距离：
d₂ = √[(10-50)² + (2-10)²] = √[1600+64] = √1664 ≈ 40.8

A距离C₁更近 → A属于簇1

客户B(12,3)：
到C₁: √[(12-10)² + (3-2)²] = √5 ≈ 2.24
到C₂: √[(12-50)² + (3-10)²] = √[1444+49] ≈ 38.6
B属于簇1

客户C(50,10)：
到C₁: √[(50-10)² + (10-2)²] = √[1600+64] ≈ 40.8
到C₂: √[(50-50)² + (10-10)²] = 0
C属于簇2

客户D(52,9)：
到C₁: √[(52-10)² + (9-2)²] = √[1764+49] ≈ 42.6
到C₂: √[(52-50)² + (9-10)²] = √[4+1] ≈ 2.24
D属于簇2

客户E(11,2)：
到C₁: √[(11-10)² + (2-2)²] = 1
到C₂: √[(11-50)² + (2-10)²] = √[1521+64] ≈ 39.8
E属于簇1

聚类结果：
簇1：{A, B, E} - 低消费低频客户
簇2：{C, D} - 高消费高频客户

更新聚类中心：
新C₁ = ((10+12+11)/3, (2+3+2)/3) = (11, 2.33)
新C₂ = ((50+52)/2, (10+9)/2) = (51, 9.5)
```

### 5.2 主成分分析 (PCA)

#### 基本原理
降维技术，将高维数据投影到低维空间，同时保留最重要的信息（最大方差）。

**核心思想**：
- 找到数据变化最大的方向（主成分）
- 用较少的主成分表示原始数据

**应用场景**：
- 数据可视化（降到2D或3D）
- 特征压缩
- 去除噪声

#### 例题12：PCA降维

**问题**：有3个学生的成绩数据

| 学生 | 数学 | 物理 | 化学 |
|------|------|------|------|
| 1    | 90   | 85   | 88   |
| 2    | 60   | 55   | 58   |
| 3    | 75   | 70   | 73   |

经过PCA分析，第一主成分PC₁代表"总体学习能力"，权重为：
- 数学：0.58
- 物理：0.58
- 化学：0.57

**问：计算每个学生在PC₁上的得分**

**解答**：
```
PC₁表示各科成绩的加权组合

学生1的PC₁得分：
PC₁ = 0.58×90 + 0.58×85 + 0.57×88
    = 52.2 + 49.3 + 50.16
    = 151.66

学生2的PC₁得分：
PC₁ = 0.58×60 + 0.58×55 + 0.57×58
    = 34.8 + 31.9 + 33.06
    = 99.76

学生3的PC₁得分：
PC₁ = 0.58×75 + 0.58×70 + 0.57×73
    = 43.5 + 40.6 + 41.61
    = 125.71

结果解释：
原来3维数据(数学、物理、化学)
降维到1维(总体学习能力)
学生1 > 学生3 > 学生2

好处：
1. 从3个特征简化为1个
2. 保留了主要信息（学习能力排名）
3. 便于可视化和分析
```

---

## 6. 强化学习

### 6.1 基本概念

**定义**：智能体(Agent)通过与环境(Environment)交互，学习最优策略以最大化累积奖励。

**核心要素**：
1. **状态 (State, S)**：环境的描述
2. **动作 (Action, A)**：智能体可执行的操作
3. **奖励 (Reward, R)**：环境对动作的反馈
4. **策略 (Policy, π)**：从状态到动作的映射
5. **价值函数 (Value Function, V)**：预期累积奖励

**工作流程**：
```
智能体观察状态 → 选择动作 → 执行动作
→ 获得奖励 → 环境变化 → 新状态 → ...
```

### 6.2 Q-Learning算法

#### 基本原理
学习一个Q值表，Q(s,a)表示在状态s下执行动作a的长期价值。

**Q值更新公式**：
```
Q(s,a) = Q(s,a) + α[r + γ×max Q(s',a') - Q(s,a)]
```
- α：学习率（0-1之间）
- r：立即奖励
- γ：折扣因子（0-1之间）
- s'：新状态
- max Q(s',a')：新状态下最佳动作的Q值

#### 例题13：机器人走迷宫

**问题**：3×3网格迷宫，机器人从起点(0,0)到终点(2,2)

```
S  .  .
.  X  .
.  .  G
```
- S：起点
- G：终点(奖励+10)
- X：障碍物(奖励-5)
- .：空地(奖励-1，鼓励快速到达)

动作：上、下、左、右
参数：α=0.5, γ=0.9

**初始Q表（全为0）**，机器人执行序列：
1. (0,0)向右→(0,1)，奖励=-1
2. (0,1)向右→(0,2)，奖励=-1
3. (0,2)向下→(1,2)，奖励=-1
4. (1,2)向下→(2,2)，奖励=+10（到达终点）

**问：更新后Q(0,0,右)的值是多少？（反向更新）**

**解答**：
```
从终点反向更新Q值

步骤4：Q(1,2,下)更新
当前：Q(1,2,下) = 0
到达终点，获得奖励r=10
Q(1,2,下) = 0 + 0.5×[10 + 0.9×0 - 0]
          = 0 + 0.5×10
          = 5

步骤3：Q(0,2,下)更新
当前：Q(0,2,下) = 0
奖励r=-1，下一状态(1,2)最佳Q值=5
Q(0,2,下) = 0 + 0.5×[-1 + 0.9×5 - 0]
          = 0 + 0.5×[-1 + 4.5]
          = 0 + 0.5×3.5
          = 1.75

步骤2：Q(0,1,右)更新
当前：Q(0,1,右) = 0
奖励r=-1，下一状态(0,2)最佳Q值=1.75
Q(0,1,右) = 0 + 0.5×[-1 + 0.9×1.75 - 0]
          = 0 + 0.5×[-1 + 1.575]
          = 0 + 0.5×0.575
          = 0.2875

步骤1：Q(0,0,右)更新
当前：Q(0,0,右) = 0
奖励r=-1，下一状态(0,1)最佳Q值=0.2875
Q(0,0,右) = 0 + 0.5×[-1 + 0.9×0.2875 - 0]
          = 0 + 0.5×[-1 + 0.259]
          = 0 + 0.5×(-0.741)
          = -0.37

答：Q(0,0,右) = -0.37

解释：虽然最终能到达终点获得高奖励，
但路径上每步都有-1的代价，
所以起点的Q值是负的。
随着多次训练，Q值会逐渐优化。
```

### 6.3 探索与利用

**探索(Exploration)**：尝试新动作，发现更好策略
**利用(Exploitation)**：选择已知最佳动作，获得高奖励

**ε-贪心策略**：
```
以概率ε随机选择动作（探索）
以概率1-ε选择最佳动作（利用）
```

#### 例题14：ε-贪心决策

**问题**：当前状态s的Q值如下
- Q(s,向左) = 2
- Q(s,向右) = 5
- Q(s,向上) = 1
- Q(s,向下) = 3

使用ε=0.2的ε-贪心策略

**问：机器人选择各动作的概率是多少？**

**解答**：
```
步骤1：找出最佳动作
最佳动作 = 向右（Q值=5最大）

步骤2：计算概率
利用概率 = 1 - ε = 1 - 0.2 = 0.8
探索概率 = ε = 0.2
探索时，4个动作等概率 = 0.2/4 = 0.05

步骤3：计算每个动作的总概率

向右（最佳动作）：
P(向右) = 0.8（利用）+ 0.05（探索）= 0.85

向左、向上、向下（非最佳动作）：
P(向左) = 0 + 0.05 = 0.05
P(向上) = 0 + 0.05 = 0.05
P(向下) = 0 + 0.05 = 0.05

验证：0.85 + 0.05 + 0.05 + 0.05 = 1.0 ✓

答：
向右：85%
向左：5%
向上：5%
向下：5%

意义：大部分时间选择最佳动作（向右），
但保留20%的时间探索，避免陷入局部最优。
```

---

## 7. 自动化应用场景

### 7.1 计算机视觉

**应用领域**：
- 图像分类（识别图片内容）
- 目标检测（定位物体位置）
- 人脸识别
- 自动驾驶

**典型技术**：CNN、YOLO、ResNet

#### 例题15：图像分类系统

**问题**：一个猫狗分类器的性能
- 测试集：1000张图片（500猫，500狗）
- 正确分类：920张
- 将猫误分为狗：30张
- 将狗误分为猫：50张

**问：计算该系统的准确率、猫类别的精确率和召回率**

**解答**：
```
首先理解数据：
实际猫：500张
  正确分类为猫：500-30 = 470张
  误分类为狗：30张

实际狗：500张
  正确分类为狗：500-50 = 450张
  误分类为猫：50张

混淆矩阵（以猫为正类）：
                预测猫   预测狗
实际猫          470      30
实际狗          50       450

1. 总体准确率
Accuracy = 920/1000 = 92%

2. 猫类别精确率
Precision_猫 = 470/(470+50) = 470/520 ≈ 0.904 = 90.4%
含义：预测为猫的图片中，90.4%真的是猫

3. 猫类别召回率
Recall_猫 = 470/(470+30) = 470/500 = 0.94 = 94%
含义：真实的猫图片中，94%被正确识别

分析：
- 准确率92%不错
- 对猫的识别（召回率94%）比对狗好
- 但有50张狗被误认为猫（可能需要更多狗的训练数据）
```

### 7.2 自然语言处理

**应用领域**：
- 机器翻译
- 文本分类（情感分析、垃圾邮件检测）
- 问答系统
- 文本生成

**典型技术**：RNN、LSTM、Transformer、BERT

#### 例题16：词向量相似度

**问题**：词向量空间中
- "国王" = [0.5, 0.8, 0.3]
- "王后" = [0.4, 0.7, 0.5]
- "男人" = [0.2, 0.3, 0.1]
- "女人" = [0.1, 0.2, 0.3]

**问：验证"国王-男人+女人≈王后"这个类比关系（使用余弦相似度）**

**解答**：
```
步骤1：计算"国王-男人+女人"
结果 = [0.5, 0.8, 0.3] - [0.2, 0.3, 0.1] + [0.1, 0.2, 0.3]
     = [0.5-0.2+0.1, 0.8-0.3+0.2, 0.3-0.1+0.3]
     = [0.4, 0.7, 0.5]

步骤2：比较与"王后"向量
"王后" = [0.4, 0.7, 0.5]
计算结果 = [0.4, 0.7, 0.5]

完全相同！

步骤3：计算余弦相似度（理论）
cos(θ) = (A·B) / (|A|×|B|)

A·B = 0.4×0.4 + 0.7×0.7 + 0.5×0.5
    = 0.16 + 0.49 + 0.25
    = 0.9

|A| = √(0.4² + 0.7² + 0.5²) = √0.9 ≈ 0.949

|B| = √(0.4² + 0.7² + 0.5²) = √0.9 ≈ 0.949

cos(θ) = 0.9 / (0.949×0.949) = 0.9 / 0.901 ≈ 0.999

答：余弦相似度≈1.0，几乎完全相同

意义：词向量捕捉了语义关系
"国王"与"男人"的关系 = "王后"与"女人"的关系
这是词向量的强大特性！
```

### 7.3 推荐系统

**应用领域**：
- 电商商品推荐
- 视频/音乐推荐
- 新闻推送

**典型方法**：
- 协同过滤
- 内容过滤
- 混合推荐

#### 例题17：协同过滤

**问题**：电影评分数据（1-5分）

| 用户 | 电影A | 电影B | 电影C | 电影D |
|------|-------|-------|-------|-------|
| 张三 | 5     | 3     | ?     | 1     |
| 李四 | 4     | ?     | 2     | 2     |
| 王五 | 5     | 4     | 1     | 1     |
| 赵六 | 1     | 2     | 5     | 4     |

**问：用基于用户的协同过滤，预测张三对电影C的评分**

**解答**：
```
步骤1：找与张三相似的用户

计算张三与其他用户的相似度（基于共同评分的电影）

张三与李四：
共同评分：A(5vs4), D(1vs2)
差异较小，相似度较高

张三与王五：
共同评分：A(5vs5), B(3vs4), D(1vs1)
非常相似！

张三与赵六：
共同评分：A(5vs1), B(3vs2), D(1vs4)
差异很大，不相似

步骤2：选择最相似用户
王五与张三最相似（评分模式接近）

步骤3：预测评分
王五对电影C的评分：1分

由于王五是高分爱好者（A=5, B=4），
但给C只打1分，说明C不符合他们的口味

张三预测评分 ≈ 1-2分

答：预测张三对电影C的评分约为1-2分（不推荐）

推荐策略：
- 推荐电影A类型的影片（张三和王五都喜欢）
- 不推荐电影C、D（都不喜欢）
```

### 7.4 工业自动化

**应用领域**：
- 故障检测
- 预测性维护
- 质量控制
- 生产优化

#### 例题18：设备故障预测

**问题**：根据传感器数据预测设备故障

历史数据：
| 温度(°C) | 振动(mm/s) | 压力(bar) | 24小时内故障? |
|----------|-----------|-----------|--------------|
| 65       | 2.1       | 5.0       | 否           |
| 85       | 4.5       | 4.8       | 是           |
| 70       | 2.8       | 5.2       | 否           |
| 90       | 5.2       | 4.5       | 是           |
| 68       | 2.3       | 5.1       | 否           |

训练出的决策树模型：
```
温度 > 75?
  是 → 故障概率 80%
  否 → 振动 > 3.0?
         是 → 故障概率 60%
         否 → 故障概率 5%
```

**问：当前设备状态：温度72°C，振动3.5mm/s，压力5.0bar，是否需要维护？**

**解答**：
```
步骤1：应用决策树

温度72°C > 75? 否
  ↓
振动3.5mm/s > 3.0? 是
  ↓
故障概率：60%

步骤2：决策
60%是较高的故障概率

建议：
1. 立即安排检查
2. 重点检查振动原因（可能是轴承磨损）
3. 准备备件

步骤3：预防措施
- 设置告警阈值：温度>75或振动>3.0
- 增加监测频率
- 记录维护后效果

答：建议进行预防性维护
故障概率60%已超过一般阈值（通常10-20%）
```

---

## 8. 实战综合题

### 综合例题19：完整的机器学习项目

**问题背景**：
某银行要建立信用卡欺诈检测系统，数据如下：

训练集（6个交易）：
| 交易金额(元) | 异地交易? | 深夜交易? | 欺诈? |
|-------------|----------|----------|-------|
| 500         | 否(0)    | 否(0)    | 否(0) |
| 10000       | 是(1)    | 是(1)    | 是(1) |
| 800         | 否(0)    | 否(0)    | 否(0) |
| 15000       | 是(1)    | 是(1)    | 是(1) |
| 600         | 否(0)    | 是(1)    | 否(0) |
| 12000       | 是(1)    | 否(1)    | 是(1) |

**任务**：
1. 使用逻辑回归，手动构建模型
2. 对新交易进行预测
3. 评估模型性能

**解答**：

```
任务1：构建逻辑回归模型

假设学习到的参数：
w₁（金额系数）= 0.0003
w₂（异地系数）= 2.5
w₃（深夜系数）= 1.8
b（偏置）= -5

模型：z = 0.0003×金额 + 2.5×异地 + 1.8×深夜 - 5
     P(欺诈) = 1/(1+e^(-z))

任务2：预测新交易

新交易：金额=8000元，异地=是(1)，深夜=否(0)

步骤1：计算z
z = 0.0003×8000 + 2.5×1 + 1.8×0 - 5
  = 2.4 + 2.5 + 0 - 5
  = -0.1

步骤2：计算概率
P(欺诈) = 1/(1+e^(0.1))
        = 1/(1+1.105)
        = 1/2.105
        ≈ 0.475 = 47.5%

步骤3：决策
使用阈值0.5
47.5% < 50% → 预测为正常交易

但47.5%接近临界值，建议：
- 发送验证短信给用户
- 暂时冻结大额转账功能
- 人工审核

任务3：模型评估

在训练集上测试（简化）：

样本1：(500, 0, 0)
z = 0.15 + 0 + 0 - 5 = -4.85
P = 1/(1+e^4.85) ≈ 0.008 → 预测：正常 ✓

样本2：(10000, 1, 1)
z = 3 + 2.5 + 1.8 - 5 = 2.3
P = 1/(1+e^(-2.3)) ≈ 0.909 → 预测：欺诈 ✓

样本6：(12000, 1, 1)
z = 3.6 + 2.5 + 0 - 5 = 1.1
P = 1/(1+e^(-1.1)) ≈ 0.750 → 预测：欺诈 ✓

训练准确率：假设6/6正确 = 100%

性能指标：
- 准确率：100%（训练集）
- 实际应用中需要在独立测试集上验证
- 需要特别关注召回率（不能漏过欺诈交易）

改进建议：
1. 收集更多数据（6个样本太少）
2. 增加特征（交易地点、商户类型、历史行为等）
3. 处理类别不平衡（欺诈交易通常很少）
4. 调整决策阈值（金融领域可能设为0.3更安全）
```

### 综合例题20：神经网络手动计算

**问题**：简单的2层神经网络

结构：
- 输入层：2个神经元 (x₁, x₂)
- 隐藏层：2个神经元 (h₁, h₂)，使用ReLU激活
- 输出层：1个神经元 (y)，使用Sigmoid激活

权重和偏置：
```
输入到隐藏层：
w₁₁=0.5, w₁₂=0.3  （到h₁）
w₂₁=-0.2, w₂₂=0.8 （到h₂）
b₁=0.1, b₂=-0.3

隐藏层到输出层：
w₃=1.0, w₄=0.5
b₃=0.2
```

输入：x₁=2, x₂=3

**问：完整的前向传播过程，计算最终输出**

**解答**：

```
步骤1：计算隐藏层

神经元h₁：
z₁ = w₁₁×x₁ + w₁₂×x₂ + b₁
   = 0.5×2 + 0.3×3 + 0.1
   = 1 + 0.9 + 0.1
   = 2.0

h₁ = ReLU(z₁) = max(0, 2.0) = 2.0

神经元h₂：
z₂ = w₂₁×x₁ + w₂₂×x₂ + b₂
   = (-0.2)×2 + 0.8×3 + (-0.3)
   = -0.4 + 2.4 - 0.3
   = 1.7

h₂ = ReLU(z₂) = max(0, 1.7) = 1.7

步骤2：计算输出层

z₃ = w₃×h₁ + w₄×h₂ + b₃
   = 1.0×2.0 + 0.5×1.7 + 0.2
   = 2.0 + 0.85 + 0.2
   = 3.05

y = Sigmoid(z₃) = 1/(1+e^(-3.05))
                = 1/(1+0.047)
                = 1/1.047
                ≈ 0.955

步骤3：结果解释

最终输出：y ≈ 0.955

如果这是二分类问题（阈值0.5）：
0.955 > 0.5 → 预测为类别1（置信度95.5%）

网络学到了什么？
- 隐藏层h₁主要响应两个输入的正向组合
- 隐藏层h₂更依赖x₂（权重0.8较大）
- 两个隐藏神经元都对最终输出有贡献
- ReLU保证了非负激活
- Sigmoid将结果映射到概率空间

如果预测错误，反向传播会调整权重：
- 计算输出误差
- 反向传递到隐藏层
- 更新所有权重和偏置
```

---

## 9. 常见错误与调试技巧

### 9.1 过拟合问题

**症状**：
- 训练准确率很高（>95%）
- 测试准确率很低（<70%）

**原因**：
- 模型过于复杂
- 训练数据太少
- 训练时间过长

**解决方法**：
1. **正则化**：在损失函数中添加惩罚项
   ```
   Loss = MSE + λ×(w₁² + w₂² + ... + wₙ²)
   ```
   λ是正则化系数

2. **Dropout**：训练时随机丢弃神经元
   ```
   以概率p（如0.5）随机将神经元输出设为0
   ```

3. **数据增强**：人工扩充训练数据
   - 图像：旋转、翻转、裁剪
   - 文本：同义词替换

4. **早停(Early Stopping)**：
   - 监控验证集性能
   - 性能不再提升时停止训练

5. **简化模型**：
   - 减少层数或神经元数量

### 9.2 梯度消失/爆炸

**症状**：
- 训练不收敛
- 损失函数变为NaN
- 权重更新过大或过小

**解决方法**：
1. **使用ReLU激活函数**（替代Sigmoid）
2. **批量归一化(Batch Normalization)**
3. **梯度裁剪**：限制梯度最大值
4. **合理初始化权重**
5. **使用残差连接(ResNet)**

### 9.3 类别不平衡

**问题**：某一类样本数量远多于其他类

例：欺诈检测中，正常交易99%，欺诈交易1%

**解决方法**：
1. **重采样**：
   - 过采样：复制少数类样本
   - 欠采样：减少多数类样本

2. **类别权重**：
   ```
   Loss = w₁×Loss_class1 + w₂×Loss_class2
   少数类设置更高权重
   ```

3. **合成样本(SMOTE)**：
   - 在少数类样本之间插值生成新样本

4. **改变评估指标**：
   - 不用准确率，用F1分数或AUC

---

## 10. 期末考试题型预测

### 题型1：概念题
- AI三个层次的区别
- 监督学习vs无监督学习
- 过拟合vs欠拟合
- 精确率vs召回率

### 题型2：计算题
- 线性回归预测
- 逻辑回归概率计算
- 神经元输出计算
- K-means聚类迭代
- Q-learning更新

### 题型3：分析题
- 给定混淆矩阵，计算评估指标
- 分析模型性能，提出改进建议
- 解释算法工作原理

### 题型4：应用题
- 选择合适的算法解决实际问题
- 设计机器学习系统
- 分析自动化场景

---

## 11. 快速记忆口诀

**机器学习三大类**：
- 监督有标签，分类和回归
- 无监督无标签，聚类和降维
- 强化有奖惩，策略要学习

**神经网络激活函数**：
- ReLU最常用，负数全归零
- Sigmoid变概率，范围零到一
- Tanh更陡峭，范围负一正一

**防止过拟合五法**：
- 数据要增多
- 正则来惩罚
- Dropout随机删
- 早停要及时
- 模型要简化

**评估指标记忆**：
- 准确率看全局
- 精确率看预测
- 召回率看实际
- F1综合评价

---

## 12. 重点公式汇总

### 线性回归
```
y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
MSE = (1/m)Σ(yᵢ - ŷᵢ)²
```

### 逻辑回归
```
P(y=1|x) = 1/(1+e^(-z))
z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
```

### 梯度下降
```
w = w - α×(∂Loss/∂w)
```

### 神经网络
```
z = Σwᵢxᵢ + b
a = activation(z)
```

### Q-Learning
```
Q(s,a) = Q(s,a) + α[r + γ×max Q(s',a') - Q(s,a)]
```

### 欧氏距离
```
d = √[(x₁-y₁)² + (x₂-y₂)² + ...]
```

### 评估指标
```
Accuracy = (TP+TN)/(TP+TN+FP+FN)
Precision = TP/(TP+FP)
Recall = TP/(TP+FN)
F1 = 2×(P×R)/(P+R)
```

---

## 13. 学习建议

1. **理解概念**：不要死记公式，理解背后原理
2. **多做练习**：手动计算例题，加深印象
3. **画图辅助**：神经网络、决策树都适合画图理解
4. **联系实际**：想想每个算法的应用场景
5. **对比记忆**：把相似概念放在一起对比（如精确率vs召回率）

---

## 14. 考前检查清单

- [ ] 能解释监督/无监督/强化学习的区别
- [ ] 会计算线性回归和逻辑回归
- [ ] 理解神经网络的前向传播
- [ ] 会计算混淆矩阵和评估指标
- [ ] 理解过拟合和解决方法
- [ ] 会进行K-means聚类迭代
- [ ] 理解Q-learning更新过程
- [ ] 能识别不同场景适用的算法
- [ ] 会分析模型性能并提出改进

---

祝您考试顺利！ 记住：理解 > 记忆，应用 > 理论！
